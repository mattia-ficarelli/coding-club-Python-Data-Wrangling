{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python-DataWrangling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhs-pycom/coding-club-Python-Data-Wrangling/blob/main/Python_DataWrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZUaB0J-VIXK"
      },
      "source": [
        "#**The NHSX Analytics Unit Python Coding Club Session 12**\n",
        "\n",
        "*Created by*: Andrew Sylvester"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4G3jbAglRSR"
      },
      "source": [
        "The goal for this session is calculate the ratio of IT and intangible assets to total assets per secondary provider over the time period FY2016/17 to FY2019/20. \n",
        "\n",
        "The content convered to do this is:\n",
        "* importing and cleaning public NHS secondary provider balance sheet data\n",
        "* appending dataframes together\n",
        "* manipulating the data to calculate key accounting items\n",
        "* reshaping it into wide format to make it easier to read"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uF5AC0cOeVc"
      },
      "source": [
        "\n",
        "#**1.Importing the data**\n",
        " \n",
        "  Start off by importing the libraries we'll use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_OTv0cGPkGn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfSgNOUyPtl0"
      },
      "source": [
        "The data for FY2017/18 for Foundation Trusts is stored at this link:\n",
        "\n",
        "https://webarchive.nationalarchives.gov.uk/ukgwa/20200327163023mp_/https:/improvement.nhs.uk/documents/3142/All_TAC_data_published_in_NHS_foundation_trusts_accounts_for_2017-18.xlsx\n",
        "\n",
        "\n",
        "The Excel file has mulitple tabs, with the main data in the \"All data\" tab, and a mapping of provdier codes to provider names in the \"List of providers\" tab. We will need both to be saved in different dataframes so that we can merge them afterwards.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd4bVSqVQaVL"
      },
      "source": [
        "URL_fts_1718 = \"https://webarchive.nationalarchives.gov.uk/ukgwa/20200327163023mp_/https:/improvement.nhs.uk/documents/3142/All_TAC_data_published_in_NHS_foundation_trusts_accounts_for_2017-18.xlsx\"\n",
        "fts_1718_df = pd.read_excel(URL_fts_1718, sheet_name = \"All data\")\n",
        "fts_1718_pcode = pd.read_excel(URL_fts_1718, sheet_name = \"List of providers\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsge3JikWAd8"
      },
      "source": [
        "# View the first rows of the accounting data\n",
        "\n",
        "fts_1718_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FusfGcGWp8U3"
      },
      "source": [
        "# View the first rows of the provider code and name mapping\n",
        "\n",
        "fts_1718_pcode.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZTFAC7FYf4I"
      },
      "source": [
        "Use [pd.merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) to merge the NHS code into the balance sheet data based on the organisation name. (hint: you will was the balance sheet data to be the left dataset and do a left merge so that any providers in the mapping which are not in this years balance sheet data are not copied across) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHtCoUAxZkNY"
      },
      "source": [
        "# Task 1: merge in the provider code to the balance sheet data\n",
        "\n",
        "fts_1718_merged = pd.merge(fts_1718_df, fts_1718_pcode, how=\"left\", left_on = \"Organisation name\", right_on = \"Full name of Provider\")\n",
        "fts_1718_merged.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSiLAM-TVjFi"
      },
      "source": [
        "When you try to append all years together later on, you will find that for some reason the FY2017/18 datasets have the organisation name variable name spelled \"Organisation name\" but the other datasets spell it \"Organisation name\". Likewise, the FY2017/18 have a variable \"Valuenumber\" which has a space in later datasets so that it is \"Value number\". \n",
        "\n",
        "To save youself some trouble later, rename these two variables in the FY2017/18 data so that they are consistent with later years' variable names. [hint: use [rename()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omJPnuIUWPCU"
      },
      "source": [
        "# Task 2: Rename the two variables to be consistent with future years variable names\n",
        "\n",
        "fts_1718_merged = fts_1718_merged.rename({\"Organisation name\": \"Organisation Name\", \"Valuenumber\" : \"Value number\"}, axis='columns')\n",
        "fts_1718_merged.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljgjAE6Y1aa"
      },
      "source": [
        "Another thing which becomes apparent when building the multiyear appended dataset is that there is no variable to identify the year of the data, so you need to add in a year identifier before appending all the datasets together. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD2c82M-ZIj-"
      },
      "source": [
        "# Task 3: Add in a variable to the merged dataframe to identify the year of this data\n",
        "\n",
        "fts_1718_merged['Year'] = \"FY2017/18\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmXBMwCDse6O"
      },
      "source": [
        "Providers which are NHS Foundation Trusts have their balance sheet data saved separately to providers with are NHS Trusts. We thus need to repeat what was done above for NHS Foundation Trusts for NHS Trusts. Furthermore, we also want to do this for Trusts and FTs for more years worth of data (FY2018/19 and FY2019/20). The urls for these files are:\n",
        "\n",
        "Trusts in FY2017/18: \n",
        "\n",
        "https://webarchive.nationalarchives.gov.uk/ukgwa/20200327163023mp_/https://improvement.nhs.uk/documents/3302/All_TAC_data_published_in_NHS_trusts_accounts_for_2017-18.xlsx\n",
        "\n",
        "Trusts in FY2018/19: \n",
        "\n",
        "https://webarchive.nationalarchives.gov.uk/ukgwa/20200327162132mp_/https:/improvement.nhs.uk/documents/5905/TAC_data_published_in_NHS_trusts_accounts_for_2018-19.xlsx\n",
        "\n",
        "Foundation Trusts in FY2018/19: \n",
        "\n",
        "https://webarchive.nationalarchives.gov.uk/ukgwa/20200327162132mp_/https:/improvement.nhs.uk/documents/5906/TAC_data_published_in_NHS_foundation_trusts_accounts_for_2018-19.xlsx\n",
        "\n",
        "Trusts in FY2019/20: \n",
        "\n",
        "https://www.england.nhs.uk/wp-content/uploads/2021/04/TAC-data-published-in-NHS-trusts-accounts-for-2019-20.xlsx\n",
        "\n",
        "Foundation Trusts in FY2019/20: \n",
        "\n",
        "https://www.england.nhs.uk/wp-content/uploads/2021/04/TAC-data-published-in-NHS-foundation-trusts-accounts-for-2019-20.xlsx\n",
        "\n",
        "The next task is to import the balance sheet and provider code mapping data from all these URLs, merge the provider code into the balance sheet data, rename \"Organisation name\" and \"Valuenumber\" where relevant, and add in a variable to each merged dataframe to identify the year. Ten points to Gryffindor if you can do this in a loop.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRiIIliuuHq4"
      },
      "source": [
        "# Task 4: Do everything above for each year & trust/FT combo\n",
        "\n",
        "dataset_names = [\"fts_1718\",\n",
        "                 \"trust_1718\", \n",
        "                 \"fts_1819\", \n",
        "                 \"trust_1819\", \n",
        "                 \"fts_1920\", \n",
        "                 \"trust_1920\"]\n",
        "\n",
        "dataset_urls = [\"https://webarchive.nationalarchives.gov.uk/ukgwa/20200327163023mp_/https:/improvement.nhs.uk/documents/3142/All_TAC_data_published_in_NHS_foundation_trusts_accounts_for_2017-18.xlsx\",\n",
        "                \"https://webarchive.nationalarchives.gov.uk/ukgwa/20200327163023mp_/https://improvement.nhs.uk/documents/3302/All_TAC_data_published_in_NHS_trusts_accounts_for_2017-18.xlsx\", \n",
        "                \"https://webarchive.nationalarchives.gov.uk/ukgwa/20200327162132mp_/https:/improvement.nhs.uk/documents/5906/TAC_data_published_in_NHS_foundation_trusts_accounts_for_2018-19.xlsx\", \n",
        "                \"https://webarchive.nationalarchives.gov.uk/ukgwa/20200327162132mp_/https:/improvement.nhs.uk/documents/5905/TAC_data_published_in_NHS_trusts_accounts_for_2018-19.xlsx\", \n",
        "                \"https://www.england.nhs.uk/wp-content/uploads/2021/04/TAC-data-published-in-NHS-foundation-trusts-accounts-for-2019-20.xlsx\", \n",
        "                \"https://www.england.nhs.uk/wp-content/uploads/2021/04/TAC-data-published-in-NHS-trusts-accounts-for-2019-20.xlsx\"]\n",
        "\n",
        "years = [\"FY2017/18\", \n",
        "         \"FY2017/18\", \n",
        "         \"FY2018/19\", \n",
        "         \"FY2018/19\", \n",
        "         \"FY2019/20\", \n",
        "         \"FY2019/20\"]\n",
        "\n",
        "df_dict = {}\n",
        "\n",
        "for i in range(0,6):\n",
        "  URL =  dataset_urls[i]\n",
        "  df_dict[dataset_names[i]+\"_df\"] = pd.read_excel(URL, sheet_name = \"All data\").rename({\"Organisation name\": \"Organisation Name\", \"Valuenumber\" : \"Value number\"}, axis='columns')\n",
        "  df_dict[dataset_names[i]+\"_pcode\"] = pd.read_excel(URL, sheet_name = \"List of providers\")\n",
        "  df_dict[dataset_names[i]+\"_merged\"] = pd.merge(df_dict[dataset_names[i]+\"_df\"], df_dict[dataset_names[i]+\"_pcode\"], how=\"left\", left_on = \"Organisation Name\", right_on = \"Full name of Provider\")\n",
        "  df_dict[dataset_names[i]+\"_merged\"]['Year'] = years[i]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H2PqX_dHTaC"
      },
      "source": [
        "df_dict['trust_1920_merged'].head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts43cuXVU9qa"
      },
      "source": [
        "# **2. Appending dataframes**\n",
        "\n",
        "The merged dataframes for each combination of trust/FT and year need to be appended into a single aggregate dataframe. [Hint: use [concat()](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8INrtmTckgr"
      },
      "source": [
        "# Task 5: build an aggreate dataframe including all the merged dataframes for each year and trust/FT combo\n",
        "frames = [df_dict['fts_1718_merged'], \n",
        "          df_dict['trust_1718_merged'], \n",
        "          df_dict['fts_1819_merged'], \n",
        "          df_dict['trust_1819_merged'], \n",
        "          df_dict['fts_1920_merged'], \n",
        "          df_dict['trust_1920_merged']]\n",
        "aggregate_df = pd.concat(frames)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG37ADZ1ifkx"
      },
      "source": [
        "Play with the data to make sure it has worked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLqdvsGodCFR"
      },
      "source": [
        "aggregate_df[[\"Year\", \"Value number\"]].groupby(\"Year\").mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCP0eU9MgZh1"
      },
      "source": [
        "We will only need a subset fo the variables for the analysis below, so might as well clean up the aggregate dataframe at this stage. [hint: use [drop()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_2wnHBTgkA8"
      },
      "source": [
        "# Task 6: Drop the variables in the Cols list below\n",
        "# Cols = [\"Region\",\"Full name of Provider\", \"Authorisation date\", \"Comments \"]\n",
        "\n",
        "cols = [\"Region\",\"Full name of Provider\", \"Authorisation date\", \"Comments \"]\n",
        "aggregate_df = aggregate_df.drop(cols, axis=1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrE3sh4VgDzl"
      },
      "source": [
        "# **3. Calculate key accounting items**\n",
        "\n",
        "The balance sheet data contains many different line items of data. The key variables we need for this analysis are:\n",
        "\n",
        "*   Opening Net Book Value (\"NBV\") of total assets\n",
        "*   Closing NBV of total assets\n",
        "*   Opening NBV of IT and intengible assets\n",
        "*   Closing NBV of IT and intangible assets\n",
        "\n",
        "To do this requires some knowledge of the accounting coding, so I have been kind enough not to make this step a task! It's worth having a look at the [.loc[ ]](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) documentation.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk0RIaekkJaH"
      },
      "source": [
        "# Create a new variable called Account_item to identify key rows of data\n",
        "aggregate_df.loc[(aggregate_df['MainCode'] == 'A14CY01') & (aggregate_df['SubCode'] == 'PPE0190'), 'Account_item'] = 'Total NBV closing value'\n",
        "aggregate_df.loc[(aggregate_df['MainCode'] == 'A14CY01') & (aggregate_df['SubCode'] == 'PPE0010'), 'Account_item'] = 'Total NBV opening value'\n",
        "aggregate_df.loc[(aggregate_df['MainCode'] == 'A14CY08') & (aggregate_df['SubCode'] == 'PPE0190'), 'Account_item'] = 'IT & Intan NBV closing value'\n",
        "aggregate_df.loc[(aggregate_df['MainCode'] == 'A14CY08') & (aggregate_df['SubCode'] == 'PPE0010'), 'Account_item'] = 'IT & Intan NBV opening value'\n",
        "aggregate_df.loc[(aggregate_df['MainCode'] == 'A13CY01') & (aggregate_df['SubCode'] == 'INT0190'), 'Account_item'] = 'IT & Intan NBV closing value'\n",
        "aggregate_df.loc[(aggregate_df['MainCode'] == 'A13CY01') & (aggregate_df['SubCode'] == 'INT0010'), 'Account_item'] = 'IT & Intan NBV opening value'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVqajwDrlXqe"
      },
      "source": [
        "aggregate_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbunIMNhllSu"
      },
      "source": [
        "# Drop all the NaNs in the Account_item variable\n",
        "aggregate_df = aggregate_df.dropna(subset=['Account_item'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et6SHzhLlqxx"
      },
      "source": [
        "aggregate_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWecojKOmgZV"
      },
      "source": [
        "We only need closing values per year and not opening values. The only reason we identified the opening values in the previous step was because the opening value for FY2017/18 is identical (expect where accountants have made adjustments) as the closing value for FY2016/17. By assuming this, we can add an extra year of closing asset values to the dataset which wasn't available from source. \n",
        "\n",
        "Use [np.where()](https://numpy.org/doc/stable/reference/generated/numpy.where.html) to change the Year to FY2016/17 wherethe Account_item is 'Total NBV opening value' or 'IT & Intan NBV opening value' and Year is FY2017/18, and change the Account_item from the opening value to closing values for rows which have been changed to FY2016/17.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLSsZQv1mJLg"
      },
      "source": [
        "# Create a row for closing values in FY2016/17 assuming they are equal to the opening values in FY2017/18\n",
        "\n",
        "aggregate_df['Year'] = np.where((aggregate_df.Year == 'FY2017/18') & (aggregate_df.Account_item == 'IT & Intan NBV opening value'), 'FY2016/17', aggregate_df.Year)\n",
        "aggregate_df['Account_item'] = np.where((aggregate_df.Year == 'FY2016/17') & (aggregate_df.Account_item == 'IT & Intan NBV opening value'), 'IT & Intan NBV closing value', aggregate_df.Account_item)\n",
        "aggregate_df['Year'] = np.where((aggregate_df.Year == 'FY2017/18') & (aggregate_df.Account_item == 'Total NBV opening value'), 'FY2016/17', aggregate_df.Year)\n",
        "aggregate_df['Account_item'] = np.where((aggregate_df.Year == 'FY2016/17') & (aggregate_df.Account_item == 'Total NBV opening value'), 'Total NBV closing value', aggregate_df.Account_item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6pE5D2hovVI"
      },
      "source": [
        "Now drop all opening values as you only kept them up until now to use as a proxy for the closing value in the year before the dataset started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dl62K0Uo7jg"
      },
      "source": [
        "# Dropping opening value rows\n",
        "\n",
        "aggregate_df = aggregate_df.drop(aggregate_df[aggregate_df.Account_item == 'IT & Intan NBV opening value'].index)\n",
        "aggregate_df = aggregate_df.drop(aggregate_df[aggregate_df.Account_item == 'Total NBV opening value'].index)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juTyawJ3pQ3h"
      },
      "source": [
        "There are multiple line items within the dataset with relate to components of each Account_item per provider and per year. The dataset needs to be collapsed on all the factor variables, and summing over the Value number in order to calculate the total value of each. [hint: use [groupby()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) and [sum()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEbJpfExpWz3"
      },
      "source": [
        "# Task 7: Calculate the total value in Value number when grouping over 'Organisation Name', 'NHS code', 'Sector', 'Year', and 'Account_item'\n",
        "\n",
        "aggregate_df = aggregate_df.groupby(['Organisation Name', 'NHS code', 'Sector', 'Year', 'Account_item'])[['Value number']].sum().reset_index()\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0st-EWjBrJhQ"
      },
      "source": [
        "# **4. Reshape wide**\n",
        "\n",
        "While long format is the business for much of the data wrangling process, it is often useful to reshape into a wide format to make the output more user friendly / human readable, or to make coding of higher level analysis easier.\n",
        "\n",
        "Use [pivot_table()](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) to reshape the dataframe so that each row has the IT & Intan NBV opening value and Total NBV opening value per provider per year.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoSNDZyUsDOj"
      },
      "source": [
        "# Task 8: Reshape the data so that each row has the IT & Intan NBV opening value and Total NBV opening value per provider per year\n",
        "\n",
        "aggregate_df = aggregate_df.pivot_table(index=['Organisation Name', 'NHS code', 'Sector', 'Year'],\n",
        "                            columns='Account_item',\n",
        "                            values='Value number').reset_index()\n",
        "\n",
        "aggregate_df.head()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRyzFzDdt7Fw"
      },
      "source": [
        "# Create a new variable for the ratio of IT and Intangible assets to Total assets\n",
        "aggregate_df['Tech_share'] = aggregate_df['IT & Intan NBV closing value']/aggregate_df['Total NBV closing value']\n",
        "\n",
        "aggregate_df.head(40)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
